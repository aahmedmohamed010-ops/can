{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8464223,"sourceType":"datasetVersion","datasetId":5046027}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## <b><span style='color:#9146ff'>|</span> Introduction </b>\n\nWelcome to this notebook on fine-tuning the Meta LLaMA-3 model on an Arabic instruct dataset using the free Kaggle recourses! 🎉\n\nIn this notebook, you will learn how to:\n\n* Set up the environment and install necessary dependencies.\n* Prepare and preprocess the Arabic dataset for model training.\n* Configure and fine-tune the Meta LLaMA-3 model.\n* Quantize the model for efficiency.\n* Use Parameter-Efficient Fine-Tuning (PEFT) with LoRA.\n* Utilize the SFT Trainer for fine-tuning.\n* Choose appropriate hyperparameters for training.\n* Test the performance of the fine-tuned model.\n\nNote : You can generalize this notebook on any other different QA instruct dataset for chatbot","metadata":{}},{"cell_type":"markdown","source":"![Llama-3](https://pc-tablet.co.in/wp-content/uploads/2024/04/Llama-3.webp)\n","metadata":{}},{"cell_type":"markdown","source":"## <b>1 <span style='color:#9146ff'>|</span> Instalation and Logging </b>","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\nsecret_label = \"HF Hub\"\nsecret_value = UserSecretsClient().get_secret(secret_label)\nlogin(token=secret_value)","metadata":{"id":"Z2f6oLvhVxsp","outputId":"b69d1788-cf20-456d-8142-2d1d30203122","execution":{"iopub.status.busy":"2024-05-21T06:04:25.415150Z","iopub.execute_input":"2024-05-21T06:04:25.416072Z","iopub.status.idle":"2024-05-21T06:04:26.149457Z","shell.execute_reply.started":"2024-05-21T06:04:25.416038Z","shell.execute_reply":"2024-05-21T06:04:26.148391Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"%pip install \\\n    datasets \\\n    evaluate \\\n    rouge_score\\\n    loralib \\\n    evaluate \\\n    accelerate \\\n    bitsandbytes \\\n    trl \\\n    peft \\\n    -U --quiet","metadata":{"id":"ra94w2yHVxsr","outputId":"af099c57-3884-4655-de17-c8fda7904785","execution":{"iopub.status.busy":"2024-05-21T06:04:28.528461Z","iopub.execute_input":"2024-05-21T06:04:28.529202Z","iopub.status.idle":"2024-05-21T06:04:51.940651Z","shell.execute_reply.started":"2024-05-21T06:04:28.529167Z","shell.execute_reply":"2024-05-21T06:04:51.939396Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\nimport torch\nimport time\nimport pandas as pd\nimport re\nimport numpy as np\nimport string\nfrom nltk.corpus import stopwords\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import TfidfTransformer,TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nimport evaluate","metadata":{"id":"Sk_aq21TVxsr","execution":{"iopub.status.busy":"2024-05-21T06:04:51.942656Z","iopub.execute_input":"2024-05-21T06:04:51.942973Z","iopub.status.idle":"2024-05-21T06:05:10.225241Z","shell.execute_reply.started":"2024-05-21T06:04:51.942946Z","shell.execute_reply":"2024-05-21T06:05:10.224413Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"2024-05-21 06:05:00.104058: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-21 06:05:00.104200: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-21 06:05:00.232632: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# !pip install -q -U git+https://github.com/huggingface/peft.git","metadata":{"id":"uWdQEEQPVxss","execution":{"iopub.status.busy":"2024-05-21T06:05:10.226297Z","iopub.execute_input":"2024-05-21T06:05:10.226880Z","iopub.status.idle":"2024-05-21T06:05:10.231098Z","shell.execute_reply.started":"2024-05-21T06:05:10.226853Z","shell.execute_reply":"2024-05-21T06:05:10.230062Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer","metadata":{"id":"gKmdS_C_Vxss","execution":{"iopub.status.busy":"2024-05-21T06:05:10.233753Z","iopub.execute_input":"2024-05-21T06:05:10.234151Z","iopub.status.idle":"2024-05-21T06:05:10.510077Z","shell.execute_reply.started":"2024-05-21T06:05:10.234113Z","shell.execute_reply":"2024-05-21T06:05:10.509289Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import transformers\n\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)","metadata":{"id":"eo-nqIZpVxss","execution":{"iopub.status.busy":"2024-05-21T06:05:10.511154Z","iopub.execute_input":"2024-05-21T06:05:10.511432Z","iopub.status.idle":"2024-05-21T06:05:10.516178Z","shell.execute_reply.started":"2024-05-21T06:05:10.511408Z","shell.execute_reply":"2024-05-21T06:05:10.515337Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## <b>2 <span style='color:#9146ff'>|</span> Model Configuration and Quantization </b>\n\n* Loading the model and its tokenizer from huggingface `AutoModelForCausalLM` library\n* Apply model quantization to reduce the size and memory usage of the model This compression technique is pivotal for deploying advanced models on devices with limited computational capabilities\n\n**Detailed Code Explanation :**\n- `AutoTokenizer`: This function loads a pre-trained tokenizer from Hugging Face's model hub.\n- `from_pretrained`: This method loads the tokenizer for the \"meta-llama/Meta-Llama-3-8B-Instruct\" model. The tokenizer is responsible for converting text into tokens that the model can process\n- `getattr`: This function dynamically gets an attribute from the `torch` module. Here, it retrieves `torch.float16`, which indicates that computations will use 16-bit floating point precision. This is typically used to reduce memory usage and increase computation speed.\n- `BitsAndBytesConfig`: This class is used to configure the quantization parameters.\n> - `load_in_4bit=True`: Indicates that the model should be loaded with 4-bit quantization. This reduces the model size and speeds up inference by using 4-bit integers instead of the usual 32-bit floating point numbers.\n> - `bnb_4bit_quant_type=\"nf4\"`: Specifies the quantization type. \"nf4\" is a specific quantization format optimized for neural network weights.\n> - `bnb_4bit_compute_dtype=compute_dtype`: Sets the computation data type to torch.float16. This means that while the model weights are stored as 4-bit integers, the computations are performed in 16-bit floating point precision.\n> - `bnb_4bit_use_double_quant=True`: Enables double quantization, which applies a second level of quantization to further reduce model size and potentially increase accuracy.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n\ncompute_dtype = getattr(torch, \"float16\")\n\nquant_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=True,\n)\n\n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n    quantization_config=quant_config,\n    device_map={\"\": 0}\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1","metadata":{"id":"o_W3l5ZPVxs0","outputId":"633493c9-0304-4b3e-b62b-5ac76b0e4dbc","execution":{"iopub.status.busy":"2024-05-21T06:05:10.517333Z","iopub.execute_input":"2024-05-21T06:05:10.517611Z","iopub.status.idle":"2024-05-21T06:07:09.100707Z","shell.execute_reply.started":"2024-05-21T06:05:10.517588Z","shell.execute_reply":"2024-05-21T06:07:09.099804Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5b0b6c8b4ee4a08a125636ea5014849"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8de8dae48b20489ab8fa6ed81d34ead2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd76a9157e0b4210aa9bcf4995e78a26"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7c09b1c41dd4cd480d09c18ff9c1a7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8aa3ca5262d14e3eb85bafdd29b16d79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3efe7b8bbe7a444683fbba26decac228"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33a9e63af1c84160aea9a67a5619112f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06f1e3beca6d4a2d94673d2098da1c1c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"127d1844af8b4194af99988d0fb14f4d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"646f11480e69437a913e073b0640fa1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6b086dfa5dc4f5898a9d286dfc5614d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28c2669d5105461eb50b5cbecc581457"}},"metadata":{}}]},{"cell_type":"code","source":"# Set pad_token as end-of-sentence token\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"id":"Id6OD1gIVxs0","execution":{"iopub.status.busy":"2024-05-21T06:07:09.102084Z","iopub.execute_input":"2024-05-21T06:07:09.102496Z","iopub.status.idle":"2024-05-21T06:07:09.107451Z","shell.execute_reply.started":"2024-05-21T06:07:09.102452Z","shell.execute_reply":"2024-05-21T06:07:09.106550Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def print_number_of_trainable_model_parameters(model):\n    trainable_model_params = 0\n    all_model_params = 0\n    for _, param in model.named_parameters():\n        all_model_params += param.numel()\n        if param.requires_grad:\n            trainable_model_params += param.numel()\n    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n\nprint(print_number_of_trainable_model_parameters(model))","metadata":{"id":"RZnrkBvKVxs1","outputId":"423d206b-2b28-4462-af8e-0f1e0ef1dba2","execution":{"iopub.status.busy":"2024-05-21T06:07:09.108640Z","iopub.execute_input":"2024-05-21T06:07:09.109417Z","iopub.status.idle":"2024-05-21T06:07:09.124111Z","shell.execute_reply.started":"2024-05-21T06:07:09.109384Z","shell.execute_reply":"2024-05-21T06:07:09.123125Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"trainable model parameters: 1050939392\nall model parameters: 4540600320\npercentage of trainable model parameters: 23.15%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## <b>3 <span style='color:#9146ff'>|</span> Data Preparation </b>\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load the Parquet file\ndf = pd.read_parquet('/kaggle/input/arabic-instruct-chatbot-dataset/train-00000-of-00001-10520e8228c2c104.parquet')\n\n# Display the first few rows\ndf.head()\n","metadata":{"id":"ELjwfgOGVxst","outputId":"b6732ecd-edb7-455b-99b8-b0271ab8fa61","execution":{"iopub.status.busy":"2024-05-21T06:07:09.125346Z","iopub.execute_input":"2024-05-21T06:07:09.125755Z","iopub.status.idle":"2024-05-21T06:07:09.479438Z","shell.execute_reply.started":"2024-05-21T06:07:09.125729Z","shell.execute_reply":"2024-05-21T06:07:09.478253Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"                              instruction input  \\\n0        أعط ثلاث نصائح للبقاء بصحة جيدة.         \n1         ما هي الألوان الثلاثة الأساسية؟         \n2                          صف بنية الذرة.         \n3           كيف يمكننا تقليل تلوث الهواء؟         \n4   صف وقتًا كان عليك فيه اتخاذ قرار صعب.         \n\n                                              output  \n0   1- تناول نظامًا غذائيًا متوازنًا وتأكد من تنا...  \n1   الألوان الثلاثة الأساسية هي الأحمر والأزرق وا...  \n2   تتكون الذرة من نواة تحتوي على البروتونات والن...  \n3  هناك عدد من الطرق للحد من تلوث الهواء ، مثل ال...  \n4  كان علي أن أتخذ قرارًا صعبًا عندما كنت أعمل كم...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>instruction</th>\n      <th>input</th>\n      <th>output</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>أعط ثلاث نصائح للبقاء بصحة جيدة.</td>\n      <td></td>\n      <td>1- تناول نظامًا غذائيًا متوازنًا وتأكد من تنا...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ما هي الألوان الثلاثة الأساسية؟</td>\n      <td></td>\n      <td>الألوان الثلاثة الأساسية هي الأحمر والأزرق وا...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>صف بنية الذرة.</td>\n      <td></td>\n      <td>تتكون الذرة من نواة تحتوي على البروتونات والن...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>كيف يمكننا تقليل تلوث الهواء؟</td>\n      <td></td>\n      <td>هناك عدد من الطرق للحد من تلوث الهواء ، مثل ال...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>صف وقتًا كان عليك فيه اتخاذ قرار صعب.</td>\n      <td></td>\n      <td>كان علي أن أتخذ قرارًا صعبًا عندما كنت أعمل كم...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## <b>4 <span style='color:#9146ff'>|</span> Data Preprocessing </b>\n","metadata":{}},{"cell_type":"code","source":"# Calculate the maximum length of text in the 'instruction' column\nmax_length_instruction = df['instruction'].apply(len).mean()\n\n# Calculate the maximum length of text in the 'output' column\nmax_length_output = df['output'].apply(len).mean()\n\n# Print the results\nprint(f\"Maximum length of 'instruction': {max_length_instruction}\")\nprint(f\"Maximum length of 'output': {max_length_output}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-21T06:07:09.483959Z","iopub.execute_input":"2024-05-21T06:07:09.484720Z","iopub.status.idle":"2024-05-21T06:07:09.576794Z","shell.execute_reply.started":"2024-05-21T06:07:09.484685Z","shell.execute_reply":"2024-05-21T06:07:09.575824Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Maximum length of 'instruction': 47.95117495480943\nMaximum length of 'output': 233.013211030345\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Detailed Code Explanation :**\n\n- `tokenizer(question, ...)`: This uses the tokenizer to convert the question string into token IDs.\n- `padding=\"max_length\"`: Pads the sequences to the maximum length specified by `max_length`.\n- `truncation=True`: Truncates the sequences if they exceed the `max_length`.\n- `max_length`: Specifies the maximum length of the tokenized sequence.\n- `return_tensors=\"pt\"`: Returns the tokenized sequences as PyTorch tensors.\n- `input_ids[0]`: Retrieves the token IDs from the tensor and assigns them to `row['input_ids']`.","metadata":{}},{"cell_type":"code","source":"def tokenize_function(row):\n    # Tokenize the conversations\n    question = ' '.join(row[\"instruction\"]) if isinstance(row[\"instruction\"], list) else row[\"instruction\"]\n\n    row['input_ids'] = tokenizer(question, padding=\"max_length\", truncation=True, max_length = 128, return_tensors=\"pt\").input_ids[0]\n    \n    # Assuming \"answer\" column is already a string, no need for conversion\n    row['labels'] = tokenizer(row[\"output\"], padding=\"max_length\", truncation=True, max_length = 256, return_tensors=\"pt\").input_ids[0]\n    \n    return row\n\n\n# Tokenize the DataFrame\ntokenized_df = df.apply(tokenize_function, axis=1)","metadata":{"id":"SWzDXZ6fVxs0","execution":{"iopub.status.busy":"2024-05-21T06:07:09.578120Z","iopub.execute_input":"2024-05-21T06:07:09.578518Z","iopub.status.idle":"2024-05-21T06:09:06.496551Z","shell.execute_reply.started":"2024-05-21T06:07:09.578485Z","shell.execute_reply":"2024-05-21T06:09:06.495672Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Convert columns to list\ntokenized_df['input_ids'] = tokenized_df['input_ids'].apply(lambda x: x.tolist())\ntokenized_df['labels'] = tokenized_df['labels'].apply(lambda x: x.tolist())","metadata":{"execution":{"iopub.status.busy":"2024-05-21T06:09:06.512757Z","iopub.execute_input":"2024-05-21T06:09:06.513116Z","iopub.status.idle":"2024-05-21T06:09:08.003671Z","shell.execute_reply.started":"2024-05-21T06:09:06.513087Z","shell.execute_reply":"2024-05-21T06:09:08.002763Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"tokenized_df","metadata":{"id":"KUC6G9sNVxs0","execution":{"iopub.status.busy":"2024-05-21T06:09:08.004868Z","iopub.execute_input":"2024-05-21T06:09:08.005163Z","iopub.status.idle":"2024-05-21T06:09:08.047709Z","shell.execute_reply.started":"2024-05-21T06:09:08.005139Z","shell.execute_reply":"2024-05-21T06:09:08.046693Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"                                             instruction  \\\n0                       أعط ثلاث نصائح للبقاء بصحة جيدة.   \n1                        ما هي الألوان الثلاثة الأساسية؟   \n2                                         صف بنية الذرة.   \n3                          كيف يمكننا تقليل تلوث الهواء؟   \n4                  صف وقتًا كان عليك فيه اتخاذ قرار صعب.   \n...                                                  ...   \n51997   قم بإنشاء مثال لما يجب أن ترغب فيه السيرة الذ...   \n51998  رتب العناصر الواردة أدناه بالترتيب لإكمال الجم...   \n51999                    اكتب فقرة تمهيدية عن شخص مشهور.   \n52000   قم بإنشاء قائمة من خمسة أشياء يجب على المرء أ...   \n52001   قم بتحليل الوثيقة القانونية المقدمة واشرح الن...   \n\n                                                   input  \\\n0                                                          \n1                                                          \n2                                                          \n3                                                          \n4                                                          \n...                                                  ...   \n51997                                                      \n51998                                      كعكة لي الأكل   \n51999                                       ميشيل أوباما   \n52000                                                      \n52001   ما يلي مقتطف من عقد بين طرفين ، بعنوان \"الشرك...   \n\n                                                  output  \\\n0       1- تناول نظامًا غذائيًا متوازنًا وتأكد من تنا...   \n1       الألوان الثلاثة الأساسية هي الأحمر والأزرق وا...   \n2       تتكون الذرة من نواة تحتوي على البروتونات والن...   \n3      هناك عدد من الطرق للحد من تلوث الهواء ، مثل ال...   \n4      كان علي أن أتخذ قرارًا صعبًا عندما كنت أعمل كم...   \n...                                                  ...   \n51997  جين تريمين \\ n1234 Main Street، Anytown، CA 98...   \n51998                                    أنا آكل الكعكة.   \n51999   ميشيل أوباما امرأة ملهمة ارتقت إلى مستوى التح...   \n52000  1. ابحث عن الفرص المحتملة وفكر مليًا في الخيار...   \n52001  تنص هذه الوثيقة القانونية على أن الشركة \"أ\" وا...   \n\n                                               input_ids  \\\n0      [128000, 106173, 44735, 117075, 118201, 100462...   \n1      [128000, 101237, 104380, 100461, 8700, 100539,...   \n2      [128000, 104477, 100829, 74541, 102554, 101341...   \n3      [128000, 114804, 106666, 101537, 40534, 101471...   \n4      [128000, 104477, 110521, 101333, 102037, 12741...   \n...                                                  ...   \n51997  [128000, 117659, 28946, 107078, 118712, 119979...   \n51998  [128000, 11318, 100936, 119424, 110732, 105155...   \n51999  [128000, 110973, 100936, 119932, 101341, 10170...   \n52000  [128000, 117659, 28946, 107078, 118712, 123797...   \n52001  [128000, 117659, 28946, 104525, 110864, 105155...   \n\n                                                  labels  \n0      [128000, 220, 16, 12, 40534, 101537, 73904, 10...  \n1      [128000, 100461, 8700, 100539, 102432, 109413,...  \n2      [128000, 112077, 103967, 102554, 101341, 64337...  \n3      [128000, 108241, 101052, 105300, 64337, 101979...  \n4      [128000, 102087, 104537, 100822, 64515, 14628,...  \n...                                                  ...  \n51997  [128000, 34190, 100327, 40534, 113690, 100327,...  \n51998  [128000, 127389, 100281, 102812, 101100, 24102...  \n51999  [128000, 102606, 33890, 96298, 64515, 100708, ...  \n52000  [128000, 16, 13, 101558, 116246, 100926, 10865...  \n52001  [128000, 102017, 42693, 104229, 105155, 85632,...  \n\n[52002 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>instruction</th>\n      <th>input</th>\n      <th>output</th>\n      <th>input_ids</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>أعط ثلاث نصائح للبقاء بصحة جيدة.</td>\n      <td></td>\n      <td>1- تناول نظامًا غذائيًا متوازنًا وتأكد من تنا...</td>\n      <td>[128000, 106173, 44735, 117075, 118201, 100462...</td>\n      <td>[128000, 220, 16, 12, 40534, 101537, 73904, 10...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ما هي الألوان الثلاثة الأساسية؟</td>\n      <td></td>\n      <td>الألوان الثلاثة الأساسية هي الأحمر والأزرق وا...</td>\n      <td>[128000, 101237, 104380, 100461, 8700, 100539,...</td>\n      <td>[128000, 100461, 8700, 100539, 102432, 109413,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>صف بنية الذرة.</td>\n      <td></td>\n      <td>تتكون الذرة من نواة تحتوي على البروتونات والن...</td>\n      <td>[128000, 104477, 100829, 74541, 102554, 101341...</td>\n      <td>[128000, 112077, 103967, 102554, 101341, 64337...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>كيف يمكننا تقليل تلوث الهواء؟</td>\n      <td></td>\n      <td>هناك عدد من الطرق للحد من تلوث الهواء ، مثل ال...</td>\n      <td>[128000, 114804, 106666, 101537, 40534, 101471...</td>\n      <td>[128000, 108241, 101052, 105300, 64337, 101979...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>صف وقتًا كان عليك فيه اتخاذ قرار صعب.</td>\n      <td></td>\n      <td>كان علي أن أتخذ قرارًا صعبًا عندما كنت أعمل كم...</td>\n      <td>[128000, 104477, 110521, 101333, 102037, 12741...</td>\n      <td>[128000, 102087, 104537, 100822, 64515, 14628,...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>51997</th>\n      <td>قم بإنشاء مثال لما يجب أن ترغب فيه السيرة الذ...</td>\n      <td></td>\n      <td>جين تريمين \\ n1234 Main Street، Anytown، CA 98...</td>\n      <td>[128000, 117659, 28946, 107078, 118712, 119979...</td>\n      <td>[128000, 34190, 100327, 40534, 113690, 100327,...</td>\n    </tr>\n    <tr>\n      <th>51998</th>\n      <td>رتب العناصر الواردة أدناه بالترتيب لإكمال الجم...</td>\n      <td>كعكة لي الأكل</td>\n      <td>أنا آكل الكعكة.</td>\n      <td>[128000, 11318, 100936, 119424, 110732, 105155...</td>\n      <td>[128000, 127389, 100281, 102812, 101100, 24102...</td>\n    </tr>\n    <tr>\n      <th>51999</th>\n      <td>اكتب فقرة تمهيدية عن شخص مشهور.</td>\n      <td>ميشيل أوباما</td>\n      <td>ميشيل أوباما امرأة ملهمة ارتقت إلى مستوى التح...</td>\n      <td>[128000, 110973, 100936, 119932, 101341, 10170...</td>\n      <td>[128000, 102606, 33890, 96298, 64515, 100708, ...</td>\n    </tr>\n    <tr>\n      <th>52000</th>\n      <td>قم بإنشاء قائمة من خمسة أشياء يجب على المرء أ...</td>\n      <td></td>\n      <td>1. ابحث عن الفرص المحتملة وفكر مليًا في الخيار...</td>\n      <td>[128000, 117659, 28946, 107078, 118712, 123797...</td>\n      <td>[128000, 16, 13, 101558, 116246, 100926, 10865...</td>\n    </tr>\n    <tr>\n      <th>52001</th>\n      <td>قم بتحليل الوثيقة القانونية المقدمة واشرح الن...</td>\n      <td>ما يلي مقتطف من عقد بين طرفين ، بعنوان \"الشرك...</td>\n      <td>تنص هذه الوثيقة القانونية على أن الشركة \"أ\" وا...</td>\n      <td>[128000, 117659, 28946, 104525, 110864, 105155...</td>\n      <td>[128000, 102017, 42693, 104229, 105155, 85632,...</td>\n    </tr>\n  </tbody>\n</table>\n<p>52002 rows × 5 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# import gc\n# torch.cuda.empty_cache()\n# gc.collect()\n# torch.cuda.empty_cache()","metadata":{"id":"j2WcXvQ9Vxs1","execution":{"iopub.status.busy":"2024-05-21T06:09:08.048998Z","iopub.execute_input":"2024-05-21T06:09:08.049337Z","iopub.status.idle":"2024-05-21T06:09:08.053779Z","shell.execute_reply.started":"2024-05-21T06:09:08.049306Z","shell.execute_reply":"2024-05-21T06:09:08.052790Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\n\n# Assuming `tokenized_df` is your pandas DataFrame\ndataset = Dataset.from_pandas(tokenized_df[:10000])","metadata":{"id":"xh2wk9PMVxs3","execution":{"iopub.status.busy":"2024-05-21T06:09:08.054885Z","iopub.execute_input":"2024-05-21T06:09:08.055177Z","iopub.status.idle":"2024-05-21T06:09:08.651367Z","shell.execute_reply.started":"2024-05-21T06:09:08.055146Z","shell.execute_reply":"2024-05-21T06:09:08.650499Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-05-21T06:09:08.652498Z","iopub.execute_input":"2024-05-21T06:09:08.652811Z","iopub.status.idle":"2024-05-21T06:09:08.659133Z","shell.execute_reply.started":"2024-05-21T06:09:08.652785Z","shell.execute_reply":"2024-05-21T06:09:08.658105Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['instruction', 'input', 'output', 'input_ids', 'labels'],\n    num_rows: 10000\n})"},"metadata":{}}]},{"cell_type":"code","source":"tokenized_datasets = dataset.map(tokenize_function)# batched=True, # batch_size=...\ntokenized_datasets = tokenized_datasets.remove_columns(['instruction', 'input','output'])","metadata":{"execution":{"iopub.status.busy":"2024-05-21T06:09:08.660357Z","iopub.execute_input":"2024-05-21T06:09:08.660745Z","iopub.status.idle":"2024-05-21T06:09:22.020921Z","shell.execute_reply.started":"2024-05-21T06:09:08.660717Z","shell.execute_reply":"2024-05-21T06:09:22.019823Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49648932dca540058685d238cf6fc86b"}},"metadata":{}}]},{"cell_type":"markdown","source":"## <b>5 <span style='color:#9146ff'>|</span> Model Training and Fine-tuning </b>\n\n### LoRA (Low-Rank Adaptation) :\nis a technique for Parameter-Efficient Fine-Tuning (PEFT) that adds trainable low-rank matrices to the model weights.\n\n![LoRa](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/133_trl_peft/step2.png)\n","metadata":{}},{"cell_type":"code","source":"# Load LoRA configuration\npeft_args = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=8,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)","metadata":{"id":"3EoA0ESJVxs3","execution":{"iopub.status.busy":"2024-05-21T06:12:32.487579Z","iopub.execute_input":"2024-05-21T06:12:32.488390Z","iopub.status.idle":"2024-05-21T06:12:32.493250Z","shell.execute_reply.started":"2024-05-21T06:12:32.488357Z","shell.execute_reply":"2024-05-21T06:12:32.492142Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"### Training Arguments :\n\n**Parameter Explanations**\n\n1. `output_dir=\"./results\"`:\nDirectory where the model checkpoints and other outputs will be saved.\nnum_train_epochs=1:\n\n2. Number of epochs to train the model. An epoch is one full pass through the training dataset.\n\n3. `per_device_train_batch_size=2`:\nBatch size per GPU/TPU core/CPU for training. This means that each device will process 2 samples per forward/backward pass.\n\n4. `gradient_accumulation_steps=1`:\nNumber of update steps to accumulate before performing a backward/update pass. This effectively increases the batch size by accumulating gradients over multiple steps.\n\n5. `optim=\"paged_adamw_32bit\"`:\nSpecifies the optimizer to use. paged_adamw_32bit is an AdamW optimizer variant that uses 32-bit precision and is optimized for memory efficiency.\n\n6. `save_steps=100`:\nNumber of steps between model checkpoint saves. The model will be saved every 100 steps.\n\n7. `logging_steps=100`:\nNumber of steps between logging outputs. Training progress will be logged every 100 steps.\n\n8. `learning_rate=2e-5`:\nInitial learning rate for the optimizer. This controls how much to adjust the model weights with respect to the loss gradient.\n\n9. `weight_decay=0.001`:\nWeight decay (L2 regularization) to apply to model parameters. Helps prevent overfitting by penalizing large weights.\n\n10. `fp16=True`:\nEnable 16-bit (half-precision) training to reduce memory usage and speed up training.\n\n11. `bf16=False`:\nDisable bfloat16 training. Bfloat16 is another 16-bit precision format, often used on TPUs.\n\n12. `max_grad_norm=0.3`:\nMaximum norm for gradient clipping. This helps prevent exploding gradients by scaling gradients that exceed this norm.\n\n13. `warmup_ratio=0.03`:\nRatio of total training steps used for linear learning rate warmup. This gradually increases the learning rate from 0 to the initial learning rate over the first 3% of the training steps.\n\n14. `group_by_length=True`:\nWhether to group sequences of roughly the same length together for training. This can improve training efficiency and stability.\n\n15. `lr_scheduler_type=\"cosine\"`:\nType of learning rate scheduler to use. \"cosine\" refers to a cosine annealing schedule, which gradually decreases the learning rate following a cosine curve.\n\n16. `report_to=\"tensorboard\"`:\nSpecifies where to report training metrics. \"tensorboard\" will log metrics to TensorBoard, a visualization tool for monitoring training.","metadata":{}},{"cell_type":"code","source":"# Set training parameters\ntraining_params = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=1,\n    per_device_train_batch_size=2,\n    # per_device_eval_batch_size=1,\n    gradient_accumulation_steps=1,\n#     evaluation_strategy=\"epoch\",\n    optim=\"paged_adamw_32bit\",\n    save_steps=100,\n    logging_steps=100,\n    learning_rate=2e-5,\n    weight_decay=0.001,\n    fp16=True,\n    bf16=False,\n    max_grad_norm=0.3,\n#     max_steps=-1,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"cosine\",\n    report_to=\"tensorboard\"\n)","metadata":{"id":"VDAagkKLVxs3","execution":{"iopub.status.busy":"2024-05-21T06:17:59.430243Z","iopub.execute_input":"2024-05-21T06:17:59.430643Z","iopub.status.idle":"2024-05-21T06:17:59.458310Z","shell.execute_reply.started":"2024-05-21T06:17:59.430590Z","shell.execute_reply":"2024-05-21T06:17:59.457474Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# # Set up training arguments\n# training_args = TrainingArguments(\n#     output_dir=\"./results\",\n#     num_train_epochs=3,\n#     per_device_train_batch_size=16,  # Adjust according to your device and global batch size\n#     gradient_accumulation_steps=2,  # Adjust according to your device and global batch size\n#     logging_dir='./logs',\n#     logging_steps=10,\n#     evaluation_strategy=\"steps\",\n#     save_steps=10,\n#     # save_total_limit=2,\n#     learning_rate=2e-5,\n#     lr_scheduler_type=\"cosine\",\n#     warmup_ratio=0.1,\n#     fp16=True,  # Use bf16 if your hardware supports it\n#     optim=\"adamw_torch_fused\",  # Use \"adamw_torch_fused\" for speedup\n#     report_to=\"tensorboard\"\n# )","metadata":{"execution":{"iopub.status.busy":"2024-05-21T06:18:00.230771Z","iopub.execute_input":"2024-05-21T06:18:00.231148Z","iopub.status.idle":"2024-05-21T06:18:00.235847Z","shell.execute_reply.started":"2024-05-21T06:18:00.231119Z","shell.execute_reply":"2024-05-21T06:18:00.234907Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"from peft import get_peft_model, TaskType\n\npeft_model = get_peft_model(model, \n                            peft_args)\nprint(print_number_of_trainable_model_parameters(peft_model))","metadata":{"execution":{"iopub.status.busy":"2024-05-21T06:18:00.909603Z","iopub.execute_input":"2024-05-21T06:18:00.909992Z","iopub.status.idle":"2024-05-21T06:18:01.027582Z","shell.execute_reply.started":"2024-05-21T06:18:00.909964Z","shell.execute_reply":"2024-05-21T06:18:01.026500Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"trainable model parameters: 3407872\nall model parameters: 4544008192\npercentage of trainable model parameters: 0.07%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### SFTTrainer:\n\n- Supervised Fine-tuning (SFT): Optimized for fine-tuning pre-trained models with smaller datasets on supervised learning tasks.\n- Simpler interface: Provides a streamlined workflow with fewer configuration options, making it easier to get started.\n- Efficient memory usage: Uses techniques like parameter-efficient (PEFT) and packing optimizations to reduce memory consumption during training.\n- Faster training: Achieves comparable or better accuracy with smaller datasets and shorter training times than Trainer.","metadata":{}},{"cell_type":"code","source":"# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=peft_model,\n    train_dataset=dataset,\n#     eval_dataset=test_dataset,\n    peft_config=peft_args,\n    dataset_text_field=\"text\",\n#     max_seq_length=256,\n    max_seq_length=None,\n    tokenizer=tokenizer,\n    args=training_params,\n    packing=False,\n)","metadata":{"id":"AT2LLausVxs3","outputId":"da975f0d-7c61-41ca-e367-7c4ee167b661","execution":{"iopub.status.busy":"2024-05-21T06:18:03.688116Z","iopub.execute_input":"2024-05-21T06:18:03.688500Z","iopub.status.idle":"2024-05-21T06:18:03.701172Z","shell.execute_reply.started":"2024-05-21T06:18:03.688471Z","shell.execute_reply":"2024-05-21T06:18:03.700128Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:246: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# # import torch_optimizer as optim\n# from transformers import AdamW\n# from transformers.optimization import get_cosine_schedule_with_warmup\n\n# # trainer.args.fsdp = \"full_shard auto_wrap\"  # Configure FSDP if required\n\n# # Initialize optimizer and scheduler\n# optimizer = AdamW(model.parameters(), lr=training_args.learning_rate)\n# num_training_steps = len(tokenized_datasets) // training_args.per_device_train_batch_size // training_args.gradient_accumulation_steps * training_args.num_train_epochs\n# lr_scheduler = get_cosine_schedule_with_warmup(\n#     optimizer,\n#     num_warmup_steps=int(0.1 * num_training_steps),\n#     num_training_steps=num_training_steps,\n# )\n\n# # Enable Flash Attention v2\n# # flash_attention_v2_enabled = True  # Assume this is integrated in your model/library\n","metadata":{"execution":{"iopub.status.busy":"2024-05-21T06:18:05.809442Z","iopub.execute_input":"2024-05-21T06:18:05.809832Z","iopub.status.idle":"2024-05-21T06:18:05.818585Z","shell.execute_reply.started":"2024-05-21T06:18:05.809805Z","shell.execute_reply":"2024-05-21T06:18:05.817605Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"### Training","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"id":"El9Ql2fhVxs3","outputId":"7ab7f6f5-0fad-4874-e061-440591152012","execution":{"iopub.status.busy":"2024-05-21T06:18:11.989153Z","iopub.execute_input":"2024-05-21T06:18:11.989816Z","iopub.status.idle":"2024-05-21T08:21:31.623833Z","shell.execute_reply.started":"2024-05-21T06:18:11.989781Z","shell.execute_reply":"2024-05-21T08:21:31.622841Z"},"trusted":true},"execution_count":52,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5000/5000 2:03:14, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>3.801900</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>3.359900</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>3.047300</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>3.014300</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>2.875900</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>2.952400</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>2.911100</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>2.810800</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>2.801300</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>2.718800</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>2.772200</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>2.772000</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>2.724300</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>2.663600</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>2.737000</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>2.709500</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>2.662100</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>2.752600</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>2.706000</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>2.698300</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>2.709000</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>2.720000</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>2.704700</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>2.639400</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>2.737500</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>2.562700</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>2.658800</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>2.565900</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>2.594100</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>2.613700</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>2.615900</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>2.620100</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>2.603500</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>2.597500</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>2.574600</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>2.598900</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>2.489700</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>2.533200</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>2.509900</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>2.552400</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>2.571400</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>2.610500</td>\n    </tr>\n    <tr>\n      <td>4300</td>\n      <td>2.502200</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>2.529500</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>2.509600</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>2.535400</td>\n    </tr>\n    <tr>\n      <td>4700</td>\n      <td>2.519000</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>2.537900</td>\n    </tr>\n    <tr>\n      <td>4900</td>\n      <td>2.519900</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>2.514800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=5000, training_loss=2.700860397338867, metrics={'train_runtime': 7395.6575, 'train_samples_per_second': 1.352, 'train_steps_per_second': 0.676, 'total_flos': 5.766399393792e+16, 'train_loss': 2.700860397338867, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"code","source":"# import gc\n# torch.cuda.empty_cache()\n# gc.collect()\n# torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-05-21T06:17:43.387721Z","iopub.execute_input":"2024-05-21T06:17:43.388509Z","iopub.status.idle":"2024-05-21T06:17:44.241267Z","shell.execute_reply.started":"2024-05-21T06:17:43.388476Z","shell.execute_reply":"2024-05-21T06:17:44.240430Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"### Save model & Publish","metadata":{}},{"cell_type":"code","source":"trainer.model.save_pretrained(\"./llama-3-8B-Arabic\")\ntokenizer.save_pretrained(\"./llama-3-8B-Arabic\")","metadata":{"id":"AC004vJZVxs3","outputId":"aba4f2c3-3ea3-48bc-ee3d-51833e91906d","execution":{"iopub.status.busy":"2024-05-21T08:42:54.664160Z","iopub.execute_input":"2024-05-21T08:42:54.664611Z","iopub.status.idle":"2024-05-21T08:42:55.328238Z","shell.execute_reply.started":"2024-05-21T08:42:54.664581Z","shell.execute_reply":"2024-05-21T08:42:55.327148Z"},"trusted":true},"execution_count":53,"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"('./llama-3-8B-Arabic/tokenizer_config.json',\n './llama-3-8B-Arabic/special_tokens_map.json',\n './llama-3-8B-Arabic/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"# model.push_to_hub(\"\")\n# tokenizer.push_to_hub(\"\")","metadata":{"execution":{"iopub.status.busy":"2024-04-29T12:00:45.016181Z","iopub.execute_input":"2024-04-29T12:00:45.016845Z","iopub.status.idle":"2024-04-29T12:03:32.291672Z","shell.execute_reply.started":"2024-04-29T12:00:45.016815Z","shell.execute_reply":"2024-04-29T12:03:32.290669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <b>6 <span style='color:#9146ff'>|</span> Testing the model performance on a single inference </b>\n","metadata":{}},{"cell_type":"code","source":"def single_inference(question):\n    messages = [\n        {\"role\": \"system\", \"content\": \"اجب علي الاتي بالعربي فقط.\"},\n    ]\n\n    messages.append({\"role\": \"user\", \"content\": question})\n\n\n    input_ids = tokenizer.apply_chat_template(\n        messages,\n        add_generation_prompt=True,\n        return_tensors=\"pt\"\n    ).to(model.device)\n\n    terminators = [\n        tokenizer.eos_token_id,\n        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n    ]\n\n    outputs = model.generate(\n        input_ids,\n        max_new_tokens=256,\n        eos_token_id=terminators,\n        do_sample=True,\n        temperature=0.4,\n    #     top_p=0.9,\n    )\n    response = outputs[0][input_ids.shape[-1]:]\n    output = tokenizer.decode(response, skip_special_tokens=True)\n    return output","metadata":{"id":"jCW22wL7Vxs1","outputId":"1f4d5ce6-fdce-4aff-f159-ac6fd2a708f3","execution":{"iopub.status.busy":"2024-05-21T08:46:34.636045Z","iopub.execute_input":"2024-05-21T08:46:34.636416Z","iopub.status.idle":"2024-05-21T08:46:34.643602Z","shell.execute_reply.started":"2024-05-21T08:46:34.636388Z","shell.execute_reply":"2024-05-21T08:46:34.642651Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"question = \"\"\"ما هي طريقة عمل البيتزا , اجب في خطوات\"\"\"\n\nanswer = single_inference(question)\n\nprint(f'INPUT QUESTION:\\n{question}')\nprint(f'\\n\\nModel Answer:\\n{answer}')","metadata":{"execution":{"iopub.status.busy":"2024-05-21T08:46:36.495532Z","iopub.execute_input":"2024-05-21T08:46:36.496444Z","iopub.status.idle":"2024-05-21T08:47:03.578922Z","shell.execute_reply.started":"2024-05-21T08:46:36.496409Z","shell.execute_reply":"2024-05-21T08:47:03.577962Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"INPUT QUESTION:\nما هي طريقة عمل البيتزا , اجب في خطوات\n\n\nModel Answer:\nلإعداد البيتزا ، اتبع الخطوات التالية:\n\n1. قم بإعداد العجينة: قم بجمع 2 كوب من الدقيق ، 1 كوب من الماء ، 1 ملعقة صغيرة من الخميرة ، 1 ملعقة صغيرة من الملح ، و 2 ملعقة صغيرة من الزيت. قم بترسيب العجينة في وعاء ، ثم قم بتغطيتها بالكفاف وتعطيتها وقتًا لترسيبها.\n2. قم بترسيب العجينة: قم بترسيب العجينة في وعاء ، ثم قم بتغطيتها بالكفاف وتعطيتها وقتًا لترسيبها.\n3. قم بترتيب المواد: قم بترتيب المواد التالية: 1 رطل من الجبن ، 1 رطل من لحم البقر ، 1 رطل من لحم الدجاج ، 1 ملعقة صغيرة من البصل ، 1 ملعقة صغيرة من الخضار ، 1 ملعقة صغيرة من الكمون ، 1 ملعقة صغيرة من الفلفل ، 1\n","output_type":"stream"}]},{"cell_type":"code","source":"question = \"\"\"   \"\"\"\n\nanswer = single_inference(question)\n\nprint(f'INPUT QUESTION:\\n{question}')\nprint(f'\\n\\nModel Answer:\\n{answer}')","metadata":{"execution":{"iopub.status.busy":"2024-04-29T12:36:26.261559Z","iopub.execute_input":"2024-04-29T12:36:26.262316Z","iopub.status.idle":"2024-04-29T12:36:26.267269Z","shell.execute_reply.started":"2024-04-29T12:36:26.262282Z","shell.execute_reply":"2024-04-29T12:36:26.26628Z"},"trusted":true},"execution_count":null,"outputs":[]}]}