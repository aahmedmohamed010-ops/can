{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8464223,"sourceType":"datasetVersion","datasetId":5046027}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## <b><span style='color:#9146ff'>|</span> Introduction </b>\n\nWelcome to this notebook on fine-tuning the Meta LLaMA-3 model on an Arabic instruct dataset using the free Kaggle recourses! ğŸ‰\n\nIn this notebook, you will learn how to:\n\n* Set up the environment and install necessary dependencies.\n* Prepare and preprocess the Arabic dataset for model training.\n* Configure and fine-tune the Meta LLaMA-3 model.\n* Quantize the model for efficiency.\n* Use Parameter-Efficient Fine-Tuning (PEFT) with LoRA.\n* Utilize the SFT Trainer for fine-tuning.\n* Choose appropriate hyperparameters for training.\n* Test the performance of the fine-tuned model.\n\nNote : You can generalize this notebook on any other different QA instruct dataset for chatbot","metadata":{}},{"cell_type":"markdown","source":"![Llama-3](https://pc-tablet.co.in/wp-content/uploads/2024/04/Llama-3.webp)\n","metadata":{}},{"cell_type":"markdown","source":"## <b>1 <span style='color:#9146ff'>|</span> Instalation and Logging </b>","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\nsecret_label = \"HF Hub\"\nsecret_value = UserSecretsClient().get_secret(secret_label)\nlogin(token=secret_value)","metadata":{"id":"Z2f6oLvhVxsp","outputId":"b69d1788-cf20-456d-8142-2d1d30203122","execution":{"iopub.status.busy":"2024-05-21T06:04:25.415150Z","iopub.execute_input":"2024-05-21T06:04:25.416072Z","iopub.status.idle":"2024-05-21T06:04:26.149457Z","shell.execute_reply.started":"2024-05-21T06:04:25.416038Z","shell.execute_reply":"2024-05-21T06:04:26.148391Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"%pip install \\\n    datasets \\\n    evaluate \\\n    rouge_score\\\n    loralib \\\n    evaluate \\\n    accelerate \\\n    bitsandbytes \\\n    trl \\\n    peft \\\n    -U --quiet","metadata":{"id":"ra94w2yHVxsr","outputId":"af099c57-3884-4655-de17-c8fda7904785","execution":{"iopub.status.busy":"2024-05-21T06:04:28.528461Z","iopub.execute_input":"2024-05-21T06:04:28.529202Z","iopub.status.idle":"2024-05-21T06:04:51.940651Z","shell.execute_reply.started":"2024-05-21T06:04:28.529167Z","shell.execute_reply":"2024-05-21T06:04:51.939396Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\nimport torch\nimport time\nimport pandas as pd\nimport re\nimport numpy as np\nimport string\nfrom nltk.corpus import stopwords\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import TfidfTransformer,TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nimport evaluate","metadata":{"id":"Sk_aq21TVxsr","execution":{"iopub.status.busy":"2024-05-21T06:04:51.942656Z","iopub.execute_input":"2024-05-21T06:04:51.942973Z","iopub.status.idle":"2024-05-21T06:05:10.225241Z","shell.execute_reply.started":"2024-05-21T06:04:51.942946Z","shell.execute_reply":"2024-05-21T06:05:10.224413Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"2024-05-21 06:05:00.104058: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-21 06:05:00.104200: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-21 06:05:00.232632: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# !pip install -q -U git+https://github.com/huggingface/peft.git","metadata":{"id":"uWdQEEQPVxss","execution":{"iopub.status.busy":"2024-05-21T06:05:10.226297Z","iopub.execute_input":"2024-05-21T06:05:10.226880Z","iopub.status.idle":"2024-05-21T06:05:10.231098Z","shell.execute_reply.started":"2024-05-21T06:05:10.226853Z","shell.execute_reply":"2024-05-21T06:05:10.230062Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer","metadata":{"id":"gKmdS_C_Vxss","execution":{"iopub.status.busy":"2024-05-21T06:05:10.233753Z","iopub.execute_input":"2024-05-21T06:05:10.234151Z","iopub.status.idle":"2024-05-21T06:05:10.510077Z","shell.execute_reply.started":"2024-05-21T06:05:10.234113Z","shell.execute_reply":"2024-05-21T06:05:10.509289Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import transformers\n\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)","metadata":{"id":"eo-nqIZpVxss","execution":{"iopub.status.busy":"2024-05-21T06:05:10.511154Z","iopub.execute_input":"2024-05-21T06:05:10.511432Z","iopub.status.idle":"2024-05-21T06:05:10.516178Z","shell.execute_reply.started":"2024-05-21T06:05:10.511408Z","shell.execute_reply":"2024-05-21T06:05:10.515337Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## <b>2 <span style='color:#9146ff'>|</span> Model Configuration and Quantization </b>\n\n* Loading the model and its tokenizer from huggingface `AutoModelForCausalLM` library\n* Apply model quantization to reduce the size and memory usage of the model This compression technique is pivotal for deploying advanced models on devices with limited computational capabilities\n\n**Detailed Code Explanation :**\n- `AutoTokenizer`: This function loads a pre-trained tokenizer from Hugging Face's model hub.\n- `from_pretrained`: This method loads the tokenizer for the \"meta-llama/Meta-Llama-3-8B-Instruct\" model. The tokenizer is responsible for converting text into tokens that the model can process\n- `getattr`: This function dynamically gets an attribute from the `torch` module. Here, it retrieves `torch.float16`, which indicates that computations will use 16-bit floating point precision. This is typically used to reduce memory usage and increase computation speed.\n- `BitsAndBytesConfig`: This class is used to configure the quantization parameters.\n> - `load_in_4bit=True`: Indicates that the model should be loaded with 4-bit quantization. This reduces the model size and speeds up inference by using 4-bit integers instead of the usual 32-bit floating point numbers.\n> - `bnb_4bit_quant_type=\"nf4\"`: Specifies the quantization type. \"nf4\" is a specific quantization format optimized for neural network weights.\n> - `bnb_4bit_compute_dtype=compute_dtype`: Sets the computation data type to torch.float16. This means that while the model weights are stored as 4-bit integers, the computations are performed in 16-bit floating point precision.\n> - `bnb_4bit_use_double_quant=True`: Enables double quantization, which applies a second level of quantization to further reduce model size and potentially increase accuracy.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n\ncompute_dtype = getattr(torch, \"float16\")\n\nquant_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=True,\n)\n\n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n    quantization_config=quant_config,\n    device_map={\"\": 0}\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1","metadata":{"id":"o_W3l5ZPVxs0","outputId":"633493c9-0304-4b3e-b62b-5ac76b0e4dbc","execution":{"iopub.status.busy":"2024-05-21T06:05:10.517333Z","iopub.execute_input":"2024-05-21T06:05:10.517611Z","iopub.status.idle":"2024-05-21T06:07:09.100707Z","shell.execute_reply.started":"2024-05-21T06:05:10.517588Z","shell.execute_reply":"2024-05-21T06:07:09.099804Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5b0b6c8b4ee4a08a125636ea5014849"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8de8dae48b20489ab8fa6ed81d34ead2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd76a9157e0b4210aa9bcf4995e78a26"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7c09b1c41dd4cd480d09c18ff9c1a7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8aa3ca5262d14e3eb85bafdd29b16d79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3efe7b8bbe7a444683fbba26decac228"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33a9e63af1c84160aea9a67a5619112f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06f1e3beca6d4a2d94673d2098da1c1c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"127d1844af8b4194af99988d0fb14f4d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"646f11480e69437a913e073b0640fa1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6b086dfa5dc4f5898a9d286dfc5614d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28c2669d5105461eb50b5cbecc581457"}},"metadata":{}}]},{"cell_type":"code","source":"# Set pad_token as end-of-sentence token\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"id":"Id6OD1gIVxs0","execution":{"iopub.status.busy":"2024-05-21T06:07:09.102084Z","iopub.execute_input":"2024-05-21T06:07:09.102496Z","iopub.status.idle":"2024-05-21T06:07:09.107451Z","shell.execute_reply.started":"2024-05-21T06:07:09.102452Z","shell.execute_reply":"2024-05-21T06:07:09.106550Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def print_number_of_trainable_model_parameters(model):\n    trainable_model_params = 0\n    all_model_params = 0\n    for _, param in model.named_parameters():\n        all_model_params += param.numel()\n        if param.requires_grad:\n            trainable_model_params += param.numel()\n    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n\nprint(print_number_of_trainable_model_parameters(model))","metadata":{"id":"RZnrkBvKVxs1","outputId":"423d206b-2b28-4462-af8e-0f1e0ef1dba2","execution":{"iopub.status.busy":"2024-05-21T06:07:09.108640Z","iopub.execute_input":"2024-05-21T06:07:09.109417Z","iopub.status.idle":"2024-05-21T06:07:09.124111Z","shell.execute_reply.started":"2024-05-21T06:07:09.109384Z","shell.execute_reply":"2024-05-21T06:07:09.123125Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"trainable model parameters: 1050939392\nall model parameters: 4540600320\npercentage of trainable model parameters: 23.15%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## <b>3 <span style='color:#9146ff'>|</span> Data Preparation </b>\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load the Parquet file\ndf = pd.read_parquet('/kaggle/input/arabic-instruct-chatbot-dataset/train-00000-of-00001-10520e8228c2c104.parquet')\n\n# Display the first few rows\ndf.head()\n","metadata":{"id":"ELjwfgOGVxst","outputId":"b6732ecd-edb7-455b-99b8-b0271ab8fa61","execution":{"iopub.status.busy":"2024-05-21T06:07:09.125346Z","iopub.execute_input":"2024-05-21T06:07:09.125755Z","iopub.status.idle":"2024-05-21T06:07:09.479438Z","shell.execute_reply.started":"2024-05-21T06:07:09.125729Z","shell.execute_reply":"2024-05-21T06:07:09.478253Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"                              instruction input  \\\n0        Ø£Ø¹Ø· Ø«Ù„Ø§Ø« Ù†ØµØ§Ø¦Ø­ Ù„Ù„Ø¨Ù‚Ø§Ø¡ Ø¨ØµØ­Ø© Ø¬ÙŠØ¯Ø©.         \n1         Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø£Ù„ÙˆØ§Ù† Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©ØŸ         \n2                          ØµÙ Ø¨Ù†ÙŠØ© Ø§Ù„Ø°Ø±Ø©.         \n3           ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†Ø§ ØªÙ‚Ù„ÙŠÙ„ ØªÙ„ÙˆØ« Ø§Ù„Ù‡ÙˆØ§Ø¡ØŸ         \n4   ØµÙ ÙˆÙ‚ØªÙ‹Ø§ ÙƒØ§Ù† Ø¹Ù„ÙŠÙƒ ÙÙŠÙ‡ Ø§ØªØ®Ø§Ø° Ù‚Ø±Ø§Ø± ØµØ¹Ø¨.         \n\n                                              output  \n0   1- ØªÙ†Ø§ÙˆÙ„ Ù†Ø¸Ø§Ù…Ù‹Ø§ ØºØ°Ø§Ø¦ÙŠÙ‹Ø§ Ù…ØªÙˆØ§Ø²Ù†Ù‹Ø§ ÙˆØªØ£ÙƒØ¯ Ù…Ù† ØªÙ†Ø§...  \n1   Ø§Ù„Ø£Ù„ÙˆØ§Ù† Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù‡ÙŠ Ø§Ù„Ø£Ø­Ù…Ø± ÙˆØ§Ù„Ø£Ø²Ø±Ù‚ ÙˆØ§...  \n2   ØªØªÙƒÙˆÙ† Ø§Ù„Ø°Ø±Ø© Ù…Ù† Ù†ÙˆØ§Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø±ÙˆØªÙˆÙ†Ø§Øª ÙˆØ§Ù„Ù†...  \n3  Ù‡Ù†Ø§Ùƒ Ø¹Ø¯Ø¯ Ù…Ù† Ø§Ù„Ø·Ø±Ù‚ Ù„Ù„Ø­Ø¯ Ù…Ù† ØªÙ„ÙˆØ« Ø§Ù„Ù‡ÙˆØ§Ø¡ ØŒ Ù…Ø«Ù„ Ø§Ù„...  \n4  ÙƒØ§Ù† Ø¹Ù„ÙŠ Ø£Ù† Ø£ØªØ®Ø° Ù‚Ø±Ø§Ø±Ù‹Ø§ ØµØ¹Ø¨Ù‹Ø§ Ø¹Ù†Ø¯Ù…Ø§ ÙƒÙ†Øª Ø£Ø¹Ù…Ù„ ÙƒÙ…...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>instruction</th>\n      <th>input</th>\n      <th>output</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Ø£Ø¹Ø· Ø«Ù„Ø§Ø« Ù†ØµØ§Ø¦Ø­ Ù„Ù„Ø¨Ù‚Ø§Ø¡ Ø¨ØµØ­Ø© Ø¬ÙŠØ¯Ø©.</td>\n      <td></td>\n      <td>1- ØªÙ†Ø§ÙˆÙ„ Ù†Ø¸Ø§Ù…Ù‹Ø§ ØºØ°Ø§Ø¦ÙŠÙ‹Ø§ Ù…ØªÙˆØ§Ø²Ù†Ù‹Ø§ ÙˆØªØ£ÙƒØ¯ Ù…Ù† ØªÙ†Ø§...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø£Ù„ÙˆØ§Ù† Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©ØŸ</td>\n      <td></td>\n      <td>Ø§Ù„Ø£Ù„ÙˆØ§Ù† Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù‡ÙŠ Ø§Ù„Ø£Ø­Ù…Ø± ÙˆØ§Ù„Ø£Ø²Ø±Ù‚ ÙˆØ§...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ØµÙ Ø¨Ù†ÙŠØ© Ø§Ù„Ø°Ø±Ø©.</td>\n      <td></td>\n      <td>ØªØªÙƒÙˆÙ† Ø§Ù„Ø°Ø±Ø© Ù…Ù† Ù†ÙˆØ§Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø±ÙˆØªÙˆÙ†Ø§Øª ÙˆØ§Ù„Ù†...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†Ø§ ØªÙ‚Ù„ÙŠÙ„ ØªÙ„ÙˆØ« Ø§Ù„Ù‡ÙˆØ§Ø¡ØŸ</td>\n      <td></td>\n      <td>Ù‡Ù†Ø§Ùƒ Ø¹Ø¯Ø¯ Ù…Ù† Ø§Ù„Ø·Ø±Ù‚ Ù„Ù„Ø­Ø¯ Ù…Ù† ØªÙ„ÙˆØ« Ø§Ù„Ù‡ÙˆØ§Ø¡ ØŒ Ù…Ø«Ù„ Ø§Ù„...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ØµÙ ÙˆÙ‚ØªÙ‹Ø§ ÙƒØ§Ù† Ø¹Ù„ÙŠÙƒ ÙÙŠÙ‡ Ø§ØªØ®Ø§Ø° Ù‚Ø±Ø§Ø± ØµØ¹Ø¨.</td>\n      <td></td>\n      <td>ÙƒØ§Ù† Ø¹Ù„ÙŠ Ø£Ù† Ø£ØªØ®Ø° Ù‚Ø±Ø§Ø±Ù‹Ø§ ØµØ¹Ø¨Ù‹Ø§ Ø¹Ù†Ø¯Ù…Ø§ ÙƒÙ†Øª Ø£Ø¹Ù…Ù„ ÙƒÙ…...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## <b>4 <span style='color:#9146ff'>|</span> Data Preprocessing </b>\n","metadata":{}},{"cell_type":"code","source":"# Calculate the maximum length of text in the 'instruction' column\nmax_length_instruction = df['instruction'].apply(len).mean()\n\n# Calculate the maximum length of text in the 'output' column\nmax_length_output = df['output'].apply(len).mean()\n\n# Print the results\nprint(f\"Maximum length of 'instruction': {max_length_instruction}\")\nprint(f\"Maximum length of 'output': {max_length_output}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-21T06:07:09.483959Z","iopub.execute_input":"2024-05-21T06:07:09.484720Z","iopub.status.idle":"2024-05-21T06:07:09.576794Z","shell.execute_reply.started":"2024-05-21T06:07:09.484685Z","shell.execute_reply":"2024-05-21T06:07:09.575824Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Maximum length of 'instruction': 47.95117495480943\nMaximum length of 'output': 233.013211030345\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Detailed Code Explanation :**\n\n- `tokenizer(question, ...)`: This uses the tokenizer to convert the question string into token IDs.\n- `padding=\"max_length\"`: Pads the sequences to the maximum length specified by `max_length`.\n- `truncation=True`: Truncates the sequences if they exceed the `max_length`.\n- `max_length`: Specifies the maximum length of the tokenized sequence.\n- `return_tensors=\"pt\"`: Returns the tokenized sequences as PyTorch tensors.\n- `input_ids[0]`: Retrieves the token IDs from the tensor and assigns them to `row['input_ids']`.","metadata":{}},{"cell_type":"code","source":"def tokenize_function(row):\n    # Tokenize the conversations\n    question = ' '.join(row[\"instruction\"]) if isinstance(row[\"instruction\"], list) else row[\"instruction\"]\n\n    row['input_ids'] = tokenizer(question, padding=\"max_length\", truncation=True, max_length = 128, return_tensors=\"pt\").input_ids[0]\n    \n    # Assuming \"answer\" column is already a string, no need for conversion\n    row['labels'] = tokenizer(row[\"output\"], padding=\"max_length\", truncation=True, max_length = 256, return_tensors=\"pt\").input_ids[0]\n    \n    return row\n\n\n# Tokenize the DataFrame\ntokenized_df = df.apply(tokenize_function, axis=1)","metadata":{"id":"SWzDXZ6fVxs0","execution":{"iopub.status.busy":"2024-05-21T06:07:09.578120Z","iopub.execute_input":"2024-05-21T06:07:09.578518Z","iopub.status.idle":"2024-05-21T06:09:06.496551Z","shell.execute_reply.started":"2024-05-21T06:07:09.578485Z","shell.execute_reply":"2024-05-21T06:09:06.495672Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Convert columns to list\ntokenized_df['input_ids'] = tokenized_df['input_ids'].apply(lambda x: x.tolist())\ntokenized_df['labels'] = tokenized_df['labels'].apply(lambda x: x.tolist())","metadata":{"execution":{"iopub.status.busy":"2024-05-21T06:09:06.512757Z","iopub.execute_input":"2024-05-21T06:09:06.513116Z","iopub.status.idle":"2024-05-21T06:09:08.003671Z","shell.execute_reply.started":"2024-05-21T06:09:06.513087Z","shell.execute_reply":"2024-05-21T06:09:08.002763Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"tokenized_df","metadata":{"id":"KUC6G9sNVxs0","execution":{"iopub.status.busy":"2024-05-21T06:09:08.004868Z","iopub.execute_input":"2024-05-21T06:09:08.005163Z","iopub.status.idle":"2024-05-21T06:09:08.047709Z","shell.execute_reply.started":"2024-05-21T06:09:08.005139Z","shell.execute_reply":"2024-05-21T06:09:08.046693Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"                                             instruction  \\\n0                       Ø£Ø¹Ø· Ø«Ù„Ø§Ø« Ù†ØµØ§Ø¦Ø­ Ù„Ù„Ø¨Ù‚Ø§Ø¡ Ø¨ØµØ­Ø© Ø¬ÙŠØ¯Ø©.   \n1                        Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø£Ù„ÙˆØ§Ù† Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©ØŸ   \n2                                         ØµÙ Ø¨Ù†ÙŠØ© Ø§Ù„Ø°Ø±Ø©.   \n3                          ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†Ø§ ØªÙ‚Ù„ÙŠÙ„ ØªÙ„ÙˆØ« Ø§Ù„Ù‡ÙˆØ§Ø¡ØŸ   \n4                  ØµÙ ÙˆÙ‚ØªÙ‹Ø§ ÙƒØ§Ù† Ø¹Ù„ÙŠÙƒ ÙÙŠÙ‡ Ø§ØªØ®Ø§Ø° Ù‚Ø±Ø§Ø± ØµØ¹Ø¨.   \n...                                                  ...   \n51997   Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø«Ø§Ù„ Ù„Ù…Ø§ ÙŠØ¬Ø¨ Ø£Ù† ØªØ±ØºØ¨ ÙÙŠÙ‡ Ø§Ù„Ø³ÙŠØ±Ø© Ø§Ù„Ø°...   \n51998  Ø±ØªØ¨ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„ÙˆØ§Ø±Ø¯Ø© Ø£Ø¯Ù†Ø§Ù‡ Ø¨Ø§Ù„ØªØ±ØªÙŠØ¨ Ù„Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„Ø¬Ù…...   \n51999                    Ø§ÙƒØªØ¨ ÙÙ‚Ø±Ø© ØªÙ…Ù‡ÙŠØ¯ÙŠØ© Ø¹Ù† Ø´Ø®Øµ Ù…Ø´Ù‡ÙˆØ±.   \n52000   Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† Ø®Ù…Ø³Ø© Ø£Ø´ÙŠØ§Ø¡ ÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø±Ø¡ Ø£...   \n52001   Ù‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØ«ÙŠÙ‚Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© ÙˆØ§Ø´Ø±Ø­ Ø§Ù„Ù†...   \n\n                                                   input  \\\n0                                                          \n1                                                          \n2                                                          \n3                                                          \n4                                                          \n...                                                  ...   \n51997                                                      \n51998                                      ÙƒØ¹ÙƒØ© Ù„ÙŠ Ø§Ù„Ø£ÙƒÙ„   \n51999                                       Ù…ÙŠØ´ÙŠÙ„ Ø£ÙˆØ¨Ø§Ù…Ø§   \n52000                                                      \n52001   Ù…Ø§ ÙŠÙ„ÙŠ Ù…Ù‚ØªØ·Ù Ù…Ù† Ø¹Ù‚Ø¯ Ø¨ÙŠÙ† Ø·Ø±ÙÙŠÙ† ØŒ Ø¨Ø¹Ù†ÙˆØ§Ù† \"Ø§Ù„Ø´Ø±Ùƒ...   \n\n                                                  output  \\\n0       1- ØªÙ†Ø§ÙˆÙ„ Ù†Ø¸Ø§Ù…Ù‹Ø§ ØºØ°Ø§Ø¦ÙŠÙ‹Ø§ Ù…ØªÙˆØ§Ø²Ù†Ù‹Ø§ ÙˆØªØ£ÙƒØ¯ Ù…Ù† ØªÙ†Ø§...   \n1       Ø§Ù„Ø£Ù„ÙˆØ§Ù† Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù‡ÙŠ Ø§Ù„Ø£Ø­Ù…Ø± ÙˆØ§Ù„Ø£Ø²Ø±Ù‚ ÙˆØ§...   \n2       ØªØªÙƒÙˆÙ† Ø§Ù„Ø°Ø±Ø© Ù…Ù† Ù†ÙˆØ§Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø±ÙˆØªÙˆÙ†Ø§Øª ÙˆØ§Ù„Ù†...   \n3      Ù‡Ù†Ø§Ùƒ Ø¹Ø¯Ø¯ Ù…Ù† Ø§Ù„Ø·Ø±Ù‚ Ù„Ù„Ø­Ø¯ Ù…Ù† ØªÙ„ÙˆØ« Ø§Ù„Ù‡ÙˆØ§Ø¡ ØŒ Ù…Ø«Ù„ Ø§Ù„...   \n4      ÙƒØ§Ù† Ø¹Ù„ÙŠ Ø£Ù† Ø£ØªØ®Ø° Ù‚Ø±Ø§Ø±Ù‹Ø§ ØµØ¹Ø¨Ù‹Ø§ Ø¹Ù†Ø¯Ù…Ø§ ÙƒÙ†Øª Ø£Ø¹Ù…Ù„ ÙƒÙ…...   \n...                                                  ...   \n51997  Ø¬ÙŠÙ† ØªØ±ÙŠÙ…ÙŠÙ† \\ n1234 Main StreetØŒ AnytownØŒ CA 98...   \n51998                                    Ø£Ù†Ø§ Ø¢ÙƒÙ„ Ø§Ù„ÙƒØ¹ÙƒØ©.   \n51999   Ù…ÙŠØ´ÙŠÙ„ Ø£ÙˆØ¨Ø§Ù…Ø§ Ø§Ù…Ø±Ø£Ø© Ù…Ù„Ù‡Ù…Ø© Ø§Ø±ØªÙ‚Øª Ø¥Ù„Ù‰ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªØ­...   \n52000  1. Ø§Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙØ±Øµ Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø© ÙˆÙÙƒØ± Ù…Ù„ÙŠÙ‹Ø§ ÙÙŠ Ø§Ù„Ø®ÙŠØ§Ø±...   \n52001  ØªÙ†Øµ Ù‡Ø°Ù‡ Ø§Ù„ÙˆØ«ÙŠÙ‚Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø¹Ù„Ù‰ Ø£Ù† Ø§Ù„Ø´Ø±ÙƒØ© \"Ø£\" ÙˆØ§...   \n\n                                               input_ids  \\\n0      [128000, 106173, 44735, 117075, 118201, 100462...   \n1      [128000, 101237, 104380, 100461, 8700, 100539,...   \n2      [128000, 104477, 100829, 74541, 102554, 101341...   \n3      [128000, 114804, 106666, 101537, 40534, 101471...   \n4      [128000, 104477, 110521, 101333, 102037, 12741...   \n...                                                  ...   \n51997  [128000, 117659, 28946, 107078, 118712, 119979...   \n51998  [128000, 11318, 100936, 119424, 110732, 105155...   \n51999  [128000, 110973, 100936, 119932, 101341, 10170...   \n52000  [128000, 117659, 28946, 107078, 118712, 123797...   \n52001  [128000, 117659, 28946, 104525, 110864, 105155...   \n\n                                                  labels  \n0      [128000, 220, 16, 12, 40534, 101537, 73904, 10...  \n1      [128000, 100461, 8700, 100539, 102432, 109413,...  \n2      [128000, 112077, 103967, 102554, 101341, 64337...  \n3      [128000, 108241, 101052, 105300, 64337, 101979...  \n4      [128000, 102087, 104537, 100822, 64515, 14628,...  \n...                                                  ...  \n51997  [128000, 34190, 100327, 40534, 113690, 100327,...  \n51998  [128000, 127389, 100281, 102812, 101100, 24102...  \n51999  [128000, 102606, 33890, 96298, 64515, 100708, ...  \n52000  [128000, 16, 13, 101558, 116246, 100926, 10865...  \n52001  [128000, 102017, 42693, 104229, 105155, 85632,...  \n\n[52002 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>instruction</th>\n      <th>input</th>\n      <th>output</th>\n      <th>input_ids</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Ø£Ø¹Ø· Ø«Ù„Ø§Ø« Ù†ØµØ§Ø¦Ø­ Ù„Ù„Ø¨Ù‚Ø§Ø¡ Ø¨ØµØ­Ø© Ø¬ÙŠØ¯Ø©.</td>\n      <td></td>\n      <td>1- ØªÙ†Ø§ÙˆÙ„ Ù†Ø¸Ø§Ù…Ù‹Ø§ ØºØ°Ø§Ø¦ÙŠÙ‹Ø§ Ù…ØªÙˆØ§Ø²Ù†Ù‹Ø§ ÙˆØªØ£ÙƒØ¯ Ù…Ù† ØªÙ†Ø§...</td>\n      <td>[128000, 106173, 44735, 117075, 118201, 100462...</td>\n      <td>[128000, 220, 16, 12, 40534, 101537, 73904, 10...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø£Ù„ÙˆØ§Ù† Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©ØŸ</td>\n      <td></td>\n      <td>Ø§Ù„Ø£Ù„ÙˆØ§Ù† Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù‡ÙŠ Ø§Ù„Ø£Ø­Ù…Ø± ÙˆØ§Ù„Ø£Ø²Ø±Ù‚ ÙˆØ§...</td>\n      <td>[128000, 101237, 104380, 100461, 8700, 100539,...</td>\n      <td>[128000, 100461, 8700, 100539, 102432, 109413,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ØµÙ Ø¨Ù†ÙŠØ© Ø§Ù„Ø°Ø±Ø©.</td>\n      <td></td>\n      <td>ØªØªÙƒÙˆÙ† Ø§Ù„Ø°Ø±Ø© Ù…Ù† Ù†ÙˆØ§Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø±ÙˆØªÙˆÙ†Ø§Øª ÙˆØ§Ù„Ù†...</td>\n      <td>[128000, 104477, 100829, 74541, 102554, 101341...</td>\n      <td>[128000, 112077, 103967, 102554, 101341, 64337...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†Ø§ ØªÙ‚Ù„ÙŠÙ„ ØªÙ„ÙˆØ« Ø§Ù„Ù‡ÙˆØ§Ø¡ØŸ</td>\n      <td></td>\n      <td>Ù‡Ù†Ø§Ùƒ Ø¹Ø¯Ø¯ Ù…Ù† Ø§Ù„Ø·Ø±Ù‚ Ù„Ù„Ø­Ø¯ Ù…Ù† ØªÙ„ÙˆØ« Ø§Ù„Ù‡ÙˆØ§Ø¡ ØŒ Ù…Ø«Ù„ Ø§Ù„...</td>\n      <td>[128000, 114804, 106666, 101537, 40534, 101471...</td>\n      <td>[128000, 108241, 101052, 105300, 64337, 101979...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ØµÙ ÙˆÙ‚ØªÙ‹Ø§ ÙƒØ§Ù† Ø¹Ù„ÙŠÙƒ ÙÙŠÙ‡ Ø§ØªØ®Ø§Ø° Ù‚Ø±Ø§Ø± ØµØ¹Ø¨.</td>\n      <td></td>\n      <td>ÙƒØ§Ù† Ø¹Ù„ÙŠ Ø£Ù† Ø£ØªØ®Ø° Ù‚Ø±Ø§Ø±Ù‹Ø§ ØµØ¹Ø¨Ù‹Ø§ Ø¹Ù†Ø¯Ù…Ø§ ÙƒÙ†Øª Ø£Ø¹Ù…Ù„ ÙƒÙ…...</td>\n      <td>[128000, 104477, 110521, 101333, 102037, 12741...</td>\n      <td>[128000, 102087, 104537, 100822, 64515, 14628,...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>51997</th>\n      <td>Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø«Ø§Ù„ Ù„Ù…Ø§ ÙŠØ¬Ø¨ Ø£Ù† ØªØ±ØºØ¨ ÙÙŠÙ‡ Ø§Ù„Ø³ÙŠØ±Ø© Ø§Ù„Ø°...</td>\n      <td></td>\n      <td>Ø¬ÙŠÙ† ØªØ±ÙŠÙ…ÙŠÙ† \\ n1234 Main StreetØŒ AnytownØŒ CA 98...</td>\n      <td>[128000, 117659, 28946, 107078, 118712, 119979...</td>\n      <td>[128000, 34190, 100327, 40534, 113690, 100327,...</td>\n    </tr>\n    <tr>\n      <th>51998</th>\n      <td>Ø±ØªØ¨ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„ÙˆØ§Ø±Ø¯Ø© Ø£Ø¯Ù†Ø§Ù‡ Ø¨Ø§Ù„ØªØ±ØªÙŠØ¨ Ù„Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„Ø¬Ù…...</td>\n      <td>ÙƒØ¹ÙƒØ© Ù„ÙŠ Ø§Ù„Ø£ÙƒÙ„</td>\n      <td>Ø£Ù†Ø§ Ø¢ÙƒÙ„ Ø§Ù„ÙƒØ¹ÙƒØ©.</td>\n      <td>[128000, 11318, 100936, 119424, 110732, 105155...</td>\n      <td>[128000, 127389, 100281, 102812, 101100, 24102...</td>\n    </tr>\n    <tr>\n      <th>51999</th>\n      <td>Ø§ÙƒØªØ¨ ÙÙ‚Ø±Ø© ØªÙ…Ù‡ÙŠØ¯ÙŠØ© Ø¹Ù† Ø´Ø®Øµ Ù…Ø´Ù‡ÙˆØ±.</td>\n      <td>Ù…ÙŠØ´ÙŠÙ„ Ø£ÙˆØ¨Ø§Ù…Ø§</td>\n      <td>Ù…ÙŠØ´ÙŠÙ„ Ø£ÙˆØ¨Ø§Ù…Ø§ Ø§Ù…Ø±Ø£Ø© Ù…Ù„Ù‡Ù…Ø© Ø§Ø±ØªÙ‚Øª Ø¥Ù„Ù‰ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªØ­...</td>\n      <td>[128000, 110973, 100936, 119932, 101341, 10170...</td>\n      <td>[128000, 102606, 33890, 96298, 64515, 100708, ...</td>\n    </tr>\n    <tr>\n      <th>52000</th>\n      <td>Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† Ø®Ù…Ø³Ø© Ø£Ø´ÙŠØ§Ø¡ ÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø±Ø¡ Ø£...</td>\n      <td></td>\n      <td>1. Ø§Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙØ±Øµ Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø© ÙˆÙÙƒØ± Ù…Ù„ÙŠÙ‹Ø§ ÙÙŠ Ø§Ù„Ø®ÙŠØ§Ø±...</td>\n      <td>[128000, 117659, 28946, 107078, 118712, 123797...</td>\n      <td>[128000, 16, 13, 101558, 116246, 100926, 10865...</td>\n    </tr>\n    <tr>\n      <th>52001</th>\n      <td>Ù‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØ«ÙŠÙ‚Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© ÙˆØ§Ø´Ø±Ø­ Ø§Ù„Ù†...</td>\n      <td>Ù…Ø§ ÙŠÙ„ÙŠ Ù…Ù‚ØªØ·Ù Ù…Ù† Ø¹Ù‚Ø¯ Ø¨ÙŠÙ† Ø·Ø±ÙÙŠÙ† ØŒ Ø¨Ø¹Ù†ÙˆØ§Ù† \"Ø§Ù„Ø´Ø±Ùƒ...</td>\n      <td>ØªÙ†Øµ Ù‡Ø°Ù‡ Ø§Ù„ÙˆØ«ÙŠÙ‚Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø¹Ù„Ù‰ Ø£Ù† Ø§Ù„Ø´Ø±ÙƒØ© \"Ø£\" ÙˆØ§...</td>\n      <td>[128000, 117659, 28946, 104525, 110864, 105155...</td>\n      <td>[128000, 102017, 42693, 104229, 105155, 85632,...</td>\n    </tr>\n  </tbody>\n</table>\n<p>52002 rows Ã— 5 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# import gc\n# torch.cuda.empty_cache()\n# gc.collect()\n# torch.cuda.empty_cache()","metadata":{"id":"j2WcXvQ9Vxs1","execution":{"iopub.status.busy":"2024-05-21T06:09:08.048998Z","iopub.execute_input":"2024-05-21T06:09:08.049337Z","iopub.status.idle":"2024-05-21T06:09:08.053779Z","shell.execute_reply.started":"2024-05-21T06:09:08.049306Z","shell.execute_reply":"2024-05-21T06:09:08.052790Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\n\n# Assuming `tokenized_df` is your pandas DataFrame\ndataset = Dataset.from_pandas(tokenized_df[:10000])","metadata":{"id":"xh2wk9PMVxs3","execution":{"iopub.status.busy":"2024-05-21T06:09:08.054885Z","iopub.execute_input":"2024-05-21T06:09:08.055177Z","iopub.status.idle":"2024-05-21T06:09:08.651367Z","shell.execute_reply.started":"2024-05-21T06:09:08.055146Z","shell.execute_reply":"2024-05-21T06:09:08.650499Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-05-21T06:09:08.652498Z","iopub.execute_input":"2024-05-21T06:09:08.652811Z","iopub.status.idle":"2024-05-21T06:09:08.659133Z","shell.execute_reply.started":"2024-05-21T06:09:08.652785Z","shell.execute_reply":"2024-05-21T06:09:08.658105Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['instruction', 'input', 'output', 'input_ids', 'labels'],\n    num_rows: 10000\n})"},"metadata":{}}]},{"cell_type":"code","source":"tokenized_datasets = dataset.map(tokenize_function)# batched=True, # batch_size=...\ntokenized_datasets = tokenized_datasets.remove_columns(['instruction', 'input','output'])","metadata":{"execution":{"iopub.status.busy":"2024-05-21T06:09:08.660357Z","iopub.execute_input":"2024-05-21T06:09:08.660745Z","iopub.status.idle":"2024-05-21T06:09:22.020921Z","shell.execute_reply.started":"2024-05-21T06:09:08.660717Z","shell.execute_reply":"2024-05-21T06:09:22.019823Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49648932dca540058685d238cf6fc86b"}},"metadata":{}}]},{"cell_type":"markdown","source":"## <b>5 <span style='color:#9146ff'>|</span> Model Training and Fine-tuning </b>\n\n### LoRA (Low-Rank Adaptation) :\nis a technique for Parameter-Efficient Fine-Tuning (PEFT) that adds trainable low-rank matrices to the model weights.\n\n![LoRa](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/133_trl_peft/step2.png)\n","metadata":{}},{"cell_type":"code","source":"# Load LoRA configuration\npeft_args = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=8,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)","metadata":{"id":"3EoA0ESJVxs3","execution":{"iopub.status.busy":"2024-05-21T06:12:32.487579Z","iopub.execute_input":"2024-05-21T06:12:32.488390Z","iopub.status.idle":"2024-05-21T06:12:32.493250Z","shell.execute_reply.started":"2024-05-21T06:12:32.488357Z","shell.execute_reply":"2024-05-21T06:12:32.492142Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"### Training Arguments :\n\n**Parameter Explanations**\n\n1. `output_dir=\"./results\"`:\nDirectory where the model checkpoints and other outputs will be saved.\nnum_train_epochs=1:\n\n2. Number of epochs to train the model. An epoch is one full pass through the training dataset.\n\n3. `per_device_train_batch_size=2`:\nBatch size per GPU/TPU core/CPU for training. This means that each device will process 2 samples per forward/backward pass.\n\n4. `gradient_accumulation_steps=1`:\nNumber of update steps to accumulate before performing a backward/update pass. This effectively increases the batch size by accumulating gradients over multiple steps.\n\n5. `optim=\"paged_adamw_32bit\"`:\nSpecifies the optimizer to use. paged_adamw_32bit is an AdamW optimizer variant that uses 32-bit precision and is optimized for memory efficiency.\n\n6. `save_steps=100`:\nNumber of steps between model checkpoint saves. The model will be saved every 100 steps.\n\n7. `logging_steps=100`:\nNumber of steps between logging outputs. Training progress will be logged every 100 steps.\n\n8. `learning_rate=2e-5`:\nInitial learning rate for the optimizer. This controls how much to adjust the model weights with respect to the loss gradient.\n\n9. `weight_decay=0.001`:\nWeight decay (L2 regularization) to apply to model parameters. Helps prevent overfitting by penalizing large weights.\n\n10. `fp16=True`:\nEnable 16-bit (half-precision) training to reduce memory usage and speed up training.\n\n11. `bf16=False`:\nDisable bfloat16 training. Bfloat16 is another 16-bit precision format, often used on TPUs.\n\n12. `max_grad_norm=0.3`:\nMaximum norm for gradient clipping. This helps prevent exploding gradients by scaling gradients that exceed this norm.\n\n13. `warmup_ratio=0.03`:\nRatio of total training steps used for linear learning rate warmup. This gradually increases the learning rate from 0 to the initial learning rate over the first 3% of the training steps.\n\n14. `group_by_length=True`:\nWhether to group sequences of roughly the same length together for training. This can improve training efficiency and stability.\n\n15. `lr_scheduler_type=\"cosine\"`:\nType of learning rate scheduler to use. \"cosine\" refers to a cosine annealing schedule, which gradually decreases the learning rate following a cosine curve.\n\n16. `report_to=\"tensorboard\"`:\nSpecifies where to report training metrics. \"tensorboard\" will log metrics to TensorBoard, a visualization tool for monitoring training.","metadata":{}},{"cell_type":"code","source":"# Set training parameters\ntraining_params = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=1,\n    per_device_train_batch_size=2,\n    # per_device_eval_batch_size=1,\n    gradient_accumulation_steps=1,\n#     evaluation_strategy=\"epoch\",\n    optim=\"paged_adamw_32bit\",\n    save_steps=100,\n    logging_steps=100,\n    learning_rate=2e-5,\n    weight_decay=0.001,\n    fp16=True,\n    bf16=False,\n    max_grad_norm=0.3,\n#     max_steps=-1,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"cosine\",\n    report_to=\"tensorboard\"\n)","metadata":{"id":"VDAagkKLVxs3","execution":{"iopub.status.busy":"2024-05-21T06:17:59.430243Z","iopub.execute_input":"2024-05-21T06:17:59.430643Z","iopub.status.idle":"2024-05-21T06:17:59.458310Z","shell.execute_reply.started":"2024-05-21T06:17:59.430590Z","shell.execute_reply":"2024-05-21T06:17:59.457474Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# # Set up training arguments\n# training_args = TrainingArguments(\n#     output_dir=\"./results\",\n#     num_train_epochs=3,\n#     per_device_train_batch_size=16,  # Adjust according to your device and global batch size\n#     gradient_accumulation_steps=2,  # Adjust according to your device and global batch size\n#     logging_dir='./logs',\n#     logging_steps=10,\n#     evaluation_strategy=\"steps\",\n#     save_steps=10,\n#     # save_total_limit=2,\n#     learning_rate=2e-5,\n#     lr_scheduler_type=\"cosine\",\n#     warmup_ratio=0.1,\n#     fp16=True,  # Use bf16 if your hardware supports it\n#     optim=\"adamw_torch_fused\",  # Use \"adamw_torch_fused\" for speedup\n#     report_to=\"tensorboard\"\n# )","metadata":{"execution":{"iopub.status.busy":"2024-05-21T06:18:00.230771Z","iopub.execute_input":"2024-05-21T06:18:00.231148Z","iopub.status.idle":"2024-05-21T06:18:00.235847Z","shell.execute_reply.started":"2024-05-21T06:18:00.231119Z","shell.execute_reply":"2024-05-21T06:18:00.234907Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"from peft import get_peft_model, TaskType\n\npeft_model = get_peft_model(model, \n                            peft_args)\nprint(print_number_of_trainable_model_parameters(peft_model))","metadata":{"execution":{"iopub.status.busy":"2024-05-21T06:18:00.909603Z","iopub.execute_input":"2024-05-21T06:18:00.909992Z","iopub.status.idle":"2024-05-21T06:18:01.027582Z","shell.execute_reply.started":"2024-05-21T06:18:00.909964Z","shell.execute_reply":"2024-05-21T06:18:01.026500Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"trainable model parameters: 3407872\nall model parameters: 4544008192\npercentage of trainable model parameters: 0.07%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### SFTTrainer:\n\n- Supervised Fine-tuning (SFT): Optimized for fine-tuning pre-trained models with smaller datasets on supervised learning tasks.\n- Simpler interface: Provides a streamlined workflow with fewer configuration options, making it easier to get started.\n- Efficient memory usage: Uses techniques like parameter-efficient (PEFT) and packing optimizations to reduce memory consumption during training.\n- Faster training: Achieves comparable or better accuracy with smaller datasets and shorter training times than Trainer.","metadata":{}},{"cell_type":"code","source":"# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=peft_model,\n    train_dataset=dataset,\n#     eval_dataset=test_dataset,\n    peft_config=peft_args,\n    dataset_text_field=\"text\",\n#     max_seq_length=256,\n    max_seq_length=None,\n    tokenizer=tokenizer,\n    args=training_params,\n    packing=False,\n)","metadata":{"id":"AT2LLausVxs3","outputId":"da975f0d-7c61-41ca-e367-7c4ee167b661","execution":{"iopub.status.busy":"2024-05-21T06:18:03.688116Z","iopub.execute_input":"2024-05-21T06:18:03.688500Z","iopub.status.idle":"2024-05-21T06:18:03.701172Z","shell.execute_reply.started":"2024-05-21T06:18:03.688471Z","shell.execute_reply":"2024-05-21T06:18:03.700128Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:246: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# # import torch_optimizer as optim\n# from transformers import AdamW\n# from transformers.optimization import get_cosine_schedule_with_warmup\n\n# # trainer.args.fsdp = \"full_shard auto_wrap\"  # Configure FSDP if required\n\n# # Initialize optimizer and scheduler\n# optimizer = AdamW(model.parameters(), lr=training_args.learning_rate)\n# num_training_steps = len(tokenized_datasets) // training_args.per_device_train_batch_size // training_args.gradient_accumulation_steps * training_args.num_train_epochs\n# lr_scheduler = get_cosine_schedule_with_warmup(\n#     optimizer,\n#     num_warmup_steps=int(0.1 * num_training_steps),\n#     num_training_steps=num_training_steps,\n# )\n\n# # Enable Flash Attention v2\n# # flash_attention_v2_enabled = True  # Assume this is integrated in your model/library\n","metadata":{"execution":{"iopub.status.busy":"2024-05-21T06:18:05.809442Z","iopub.execute_input":"2024-05-21T06:18:05.809832Z","iopub.status.idle":"2024-05-21T06:18:05.818585Z","shell.execute_reply.started":"2024-05-21T06:18:05.809805Z","shell.execute_reply":"2024-05-21T06:18:05.817605Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"### Training","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"id":"El9Ql2fhVxs3","outputId":"7ab7f6f5-0fad-4874-e061-440591152012","execution":{"iopub.status.busy":"2024-05-21T06:18:11.989153Z","iopub.execute_input":"2024-05-21T06:18:11.989816Z","iopub.status.idle":"2024-05-21T08:21:31.623833Z","shell.execute_reply.started":"2024-05-21T06:18:11.989781Z","shell.execute_reply":"2024-05-21T08:21:31.622841Z"},"trusted":true},"execution_count":52,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5000/5000 2:03:14, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>3.801900</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>3.359900</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>3.047300</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>3.014300</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>2.875900</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>2.952400</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>2.911100</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>2.810800</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>2.801300</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>2.718800</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>2.772200</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>2.772000</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>2.724300</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>2.663600</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>2.737000</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>2.709500</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>2.662100</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>2.752600</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>2.706000</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>2.698300</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>2.709000</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>2.720000</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>2.704700</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>2.639400</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>2.737500</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>2.562700</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>2.658800</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>2.565900</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>2.594100</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>2.613700</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>2.615900</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>2.620100</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>2.603500</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>2.597500</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>2.574600</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>2.598900</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>2.489700</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>2.533200</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>2.509900</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>2.552400</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>2.571400</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>2.610500</td>\n    </tr>\n    <tr>\n      <td>4300</td>\n      <td>2.502200</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>2.529500</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>2.509600</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>2.535400</td>\n    </tr>\n    <tr>\n      <td>4700</td>\n      <td>2.519000</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>2.537900</td>\n    </tr>\n    <tr>\n      <td>4900</td>\n      <td>2.519900</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>2.514800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=5000, training_loss=2.700860397338867, metrics={'train_runtime': 7395.6575, 'train_samples_per_second': 1.352, 'train_steps_per_second': 0.676, 'total_flos': 5.766399393792e+16, 'train_loss': 2.700860397338867, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"code","source":"# import gc\n# torch.cuda.empty_cache()\n# gc.collect()\n# torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-05-21T06:17:43.387721Z","iopub.execute_input":"2024-05-21T06:17:43.388509Z","iopub.status.idle":"2024-05-21T06:17:44.241267Z","shell.execute_reply.started":"2024-05-21T06:17:43.388476Z","shell.execute_reply":"2024-05-21T06:17:44.240430Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"### Save model & Publish","metadata":{}},{"cell_type":"code","source":"trainer.model.save_pretrained(\"./llama-3-8B-Arabic\")\ntokenizer.save_pretrained(\"./llama-3-8B-Arabic\")","metadata":{"id":"AC004vJZVxs3","outputId":"aba4f2c3-3ea3-48bc-ee3d-51833e91906d","execution":{"iopub.status.busy":"2024-05-21T08:42:54.664160Z","iopub.execute_input":"2024-05-21T08:42:54.664611Z","iopub.status.idle":"2024-05-21T08:42:55.328238Z","shell.execute_reply.started":"2024-05-21T08:42:54.664581Z","shell.execute_reply":"2024-05-21T08:42:55.327148Z"},"trusted":true},"execution_count":53,"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"('./llama-3-8B-Arabic/tokenizer_config.json',\n './llama-3-8B-Arabic/special_tokens_map.json',\n './llama-3-8B-Arabic/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"# model.push_to_hub(\"\")\n# tokenizer.push_to_hub(\"\")","metadata":{"execution":{"iopub.status.busy":"2024-04-29T12:00:45.016181Z","iopub.execute_input":"2024-04-29T12:00:45.016845Z","iopub.status.idle":"2024-04-29T12:03:32.291672Z","shell.execute_reply.started":"2024-04-29T12:00:45.016815Z","shell.execute_reply":"2024-04-29T12:03:32.290669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <b>6 <span style='color:#9146ff'>|</span> Testing the model performance on a single inference </b>\n","metadata":{}},{"cell_type":"code","source":"def single_inference(question):\n    messages = [\n        {\"role\": \"system\", \"content\": \"Ø§Ø¬Ø¨ Ø¹Ù„ÙŠ Ø§Ù„Ø§ØªÙŠ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠ ÙÙ‚Ø·.\"},\n    ]\n\n    messages.append({\"role\": \"user\", \"content\": question})\n\n\n    input_ids = tokenizer.apply_chat_template(\n        messages,\n        add_generation_prompt=True,\n        return_tensors=\"pt\"\n    ).to(model.device)\n\n    terminators = [\n        tokenizer.eos_token_id,\n        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n    ]\n\n    outputs = model.generate(\n        input_ids,\n        max_new_tokens=256,\n        eos_token_id=terminators,\n        do_sample=True,\n        temperature=0.4,\n    #     top_p=0.9,\n    )\n    response = outputs[0][input_ids.shape[-1]:]\n    output = tokenizer.decode(response, skip_special_tokens=True)\n    return output","metadata":{"id":"jCW22wL7Vxs1","outputId":"1f4d5ce6-fdce-4aff-f159-ac6fd2a708f3","execution":{"iopub.status.busy":"2024-05-21T08:46:34.636045Z","iopub.execute_input":"2024-05-21T08:46:34.636416Z","iopub.status.idle":"2024-05-21T08:46:34.643602Z","shell.execute_reply.started":"2024-05-21T08:46:34.636388Z","shell.execute_reply":"2024-05-21T08:46:34.642651Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"question = \"\"\"Ù…Ø§ Ù‡ÙŠ Ø·Ø±ÙŠÙ‚Ø© Ø¹Ù…Ù„ Ø§Ù„Ø¨ÙŠØªØ²Ø§ , Ø§Ø¬Ø¨ ÙÙŠ Ø®Ø·ÙˆØ§Øª\"\"\"\n\nanswer = single_inference(question)\n\nprint(f'INPUT QUESTION:\\n{question}')\nprint(f'\\n\\nModel Answer:\\n{answer}')","metadata":{"execution":{"iopub.status.busy":"2024-05-21T08:46:36.495532Z","iopub.execute_input":"2024-05-21T08:46:36.496444Z","iopub.status.idle":"2024-05-21T08:47:03.578922Z","shell.execute_reply.started":"2024-05-21T08:46:36.496409Z","shell.execute_reply":"2024-05-21T08:47:03.577962Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"INPUT QUESTION:\nÙ…Ø§ Ù‡ÙŠ Ø·Ø±ÙŠÙ‚Ø© Ø¹Ù…Ù„ Ø§Ù„Ø¨ÙŠØªØ²Ø§ , Ø§Ø¬Ø¨ ÙÙŠ Ø®Ø·ÙˆØ§Øª\n\n\nModel Answer:\nÙ„Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØªØ²Ø§ ØŒ Ø§ØªØ¨Ø¹ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©:\n\n1. Ù‚Ù… Ø¨Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¹Ø¬ÙŠÙ†Ø©: Ù‚Ù… Ø¨Ø¬Ù…Ø¹ 2 ÙƒÙˆØ¨ Ù…Ù† Ø§Ù„Ø¯Ù‚ÙŠÙ‚ ØŒ 1 ÙƒÙˆØ¨ Ù…Ù† Ø§Ù„Ù…Ø§Ø¡ ØŒ 1 Ù…Ù„Ø¹Ù‚Ø© ØµØºÙŠØ±Ø© Ù…Ù† Ø§Ù„Ø®Ù…ÙŠØ±Ø© ØŒ 1 Ù…Ù„Ø¹Ù‚Ø© ØµØºÙŠØ±Ø© Ù…Ù† Ø§Ù„Ù…Ù„Ø­ ØŒ Ùˆ 2 Ù…Ù„Ø¹Ù‚Ø© ØµØºÙŠØ±Ø© Ù…Ù† Ø§Ù„Ø²ÙŠØª. Ù‚Ù… Ø¨ØªØ±Ø³ÙŠØ¨ Ø§Ù„Ø¹Ø¬ÙŠÙ†Ø© ÙÙŠ ÙˆØ¹Ø§Ø¡ ØŒ Ø«Ù… Ù‚Ù… Ø¨ØªØºØ·ÙŠØªÙ‡Ø§ Ø¨Ø§Ù„ÙƒÙØ§Ù ÙˆØªØ¹Ø·ÙŠØªÙ‡Ø§ ÙˆÙ‚ØªÙ‹Ø§ Ù„ØªØ±Ø³ÙŠØ¨Ù‡Ø§.\n2. Ù‚Ù… Ø¨ØªØ±Ø³ÙŠØ¨ Ø§Ù„Ø¹Ø¬ÙŠÙ†Ø©: Ù‚Ù… Ø¨ØªØ±Ø³ÙŠØ¨ Ø§Ù„Ø¹Ø¬ÙŠÙ†Ø© ÙÙŠ ÙˆØ¹Ø§Ø¡ ØŒ Ø«Ù… Ù‚Ù… Ø¨ØªØºØ·ÙŠØªÙ‡Ø§ Ø¨Ø§Ù„ÙƒÙØ§Ù ÙˆØªØ¹Ø·ÙŠØªÙ‡Ø§ ÙˆÙ‚ØªÙ‹Ø§ Ù„ØªØ±Ø³ÙŠØ¨Ù‡Ø§.\n3. Ù‚Ù… Ø¨ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…ÙˆØ§Ø¯: Ù‚Ù… Ø¨ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„ØªØ§Ù„ÙŠØ©: 1 Ø±Ø·Ù„ Ù…Ù† Ø§Ù„Ø¬Ø¨Ù† ØŒ 1 Ø±Ø·Ù„ Ù…Ù† Ù„Ø­Ù… Ø§Ù„Ø¨Ù‚Ø± ØŒ 1 Ø±Ø·Ù„ Ù…Ù† Ù„Ø­Ù… Ø§Ù„Ø¯Ø¬Ø§Ø¬ ØŒ 1 Ù…Ù„Ø¹Ù‚Ø© ØµØºÙŠØ±Ø© Ù…Ù† Ø§Ù„Ø¨ØµÙ„ ØŒ 1 Ù…Ù„Ø¹Ù‚Ø© ØµØºÙŠØ±Ø© Ù…Ù† Ø§Ù„Ø®Ø¶Ø§Ø± ØŒ 1 Ù…Ù„Ø¹Ù‚Ø© ØµØºÙŠØ±Ø© Ù…Ù† Ø§Ù„ÙƒÙ…ÙˆÙ† ØŒ 1 Ù…Ù„Ø¹Ù‚Ø© ØµØºÙŠØ±Ø© Ù…Ù† Ø§Ù„ÙÙ„ÙÙ„ ØŒ 1\n","output_type":"stream"}]},{"cell_type":"code","source":"question = \"\"\"   \"\"\"\n\nanswer = single_inference(question)\n\nprint(f'INPUT QUESTION:\\n{question}')\nprint(f'\\n\\nModel Answer:\\n{answer}')","metadata":{"execution":{"iopub.status.busy":"2024-04-29T12:36:26.261559Z","iopub.execute_input":"2024-04-29T12:36:26.262316Z","iopub.status.idle":"2024-04-29T12:36:26.267269Z","shell.execute_reply.started":"2024-04-29T12:36:26.262282Z","shell.execute_reply":"2024-04-29T12:36:26.26628Z"},"trusted":true},"execution_count":null,"outputs":[]}]}